{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building - Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./data/data_train_borderline_smote.csv\")\n",
    "df_val = pd.read_csv(\"./data/data_val.csv\")\n",
    "df_test = pd.read_csv(\"./data/data_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:, 1:].values\n",
    "y_train = df_train.iloc[:, 0].values\n",
    "X_val = df_val.iloc[:, 1:].values\n",
    "y_val = df_val.iloc[:, 0].values\n",
    "X_test = df_test.iloc[:, 1:].values\n",
    "y_test = df_test.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train.shape[1],)))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\", AUC(name=\"roc_auc\")],\n",
    "    optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Results:\n",
      "Training AUC: 0.9301, Accuracy: 0.841\n",
      "Validation AUC: 0.7919, Accuracy: 0.799\n"
     ]
    }
   ],
   "source": [
    "training_auc = history.history[\"roc_auc\"][-1]\n",
    "validation_auc = history.history[\"val_roc_auc\"][-1]\n",
    "training_accuracy = history.history[\"accuracy\"][-1]\n",
    "validation_accuracy = history.history[\"val_accuracy\"][-1]\n",
    "print(\"Baseline Model Results:\")\n",
    "print(f\"Training AUC: {round(training_auc, 4)}, Accuracy: {round(training_accuracy, 4)}\")\n",
    "print(f\"Validation AUC: {round(validation_auc, 4)}, Accuracy: {round(validation_accuracy, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Xie's 2019 research paper reports that the neural network achieved an AUC of 0.7949 and an accuracy of 0.8241.\n",
    "* Assume the optimal performance is 0.7949.\n",
    "* Since the training AUC (0.9301) is larger than the optimal performance, we don't have the problem of high avoidable bias. \n",
    "* Instead, there is a larger gap between training AUC (0.9301) and validation AUC (0.7919), the model has the problem of high variance.\n",
    "* Before adding regularisation to reduce the variance, the model architecture such as number of layers and units and learning rate were tuned. This also help reduce variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('./hide/training history/baseline_model_v2.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection / tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce avoidable bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1],)))\n",
    "    for i in range(hp.Int(\"num_layers\", 2, 4)):\n",
    "        model.add(Dense(\n",
    "            hp.Int(f\"units_{i}\", min_value=32, max_value=128, step=32),activation=\"relu\"\n",
    "            )\n",
    "        )\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\", AUC(name=\"auc\")],\n",
    "        optimizer=Adam(\n",
    "            learning_rate=hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\"),\n",
    "            beta_1=0.9, beta_2=0.999, epsilon=1e-08\n",
    "            )\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=kt.Objective(\"val_auc\", direction=\"max\"),\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"hide/training history\",\n",
    "    project_name=\"train_bigger_model_v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 4\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 4, 'step': 1, 'sampling': 'linear'}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
      "units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 05m 33s]\n",
      "val_auc: 0.8140186071395874\n",
      "\n",
      "Best val_auc So Far: 0.8144325613975525\n",
      "Total elapsed time: 01h 04m 32s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=100, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best NN architecture: {'num_layers': 2, 'units_0': 128, 'units_1': 128, 'learning_rate': 0.004782203154550918, 'units_2': 96, 'units_3': 32}\n"
     ]
    }
   ],
   "source": [
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"The best NN architecture: {best_hp.values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in hide/training history/train_bigger_model_v2\n",
      "Showing 1 best trials\n",
      "Objective(name=\"val_auc\", direction=\"max\")\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 128\n",
      "units_1: 128\n",
      "learning_rate: 0.004782203154550918\n",
      "units_2: 96\n",
      "units_3: 32\n",
      "Score: 0.8144325613975525\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "history = best_model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc = history.history[\"auc\"][-1]\n",
    "val_auc = history.history[\"val_auc\"][-1]\n",
    "train_acc = history.history[\"accuracy\"][-1]\n",
    "val_acc = history.history[\"val_accuracy\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.9226, Accuracy: 0.8326\n",
      "Validation AUC: 0.8038, Accuracy: 0.8006\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training AUC: {round(train_auc, 4)}, Accuracy: {round(train_acc, 4)}\")\n",
    "print(f\"Validation AUC: {round(val_auc, 4)}, Accuracy: {round(val_acc, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp): \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(128, activation=\"relu\", kernel_regularizer=l2(hp.Float(\"l2_0\", min_value=1e-5, max_value=1e-2, sampling=\"log\"))))\n",
    "    model.add(Dense(128, activation=\"relu\", kernel_regularizer=l2(hp.Float(\"l2_1\", min_value=1e-5, max_value=1e-2, sampling=\"log\"))))\n",
    "    model.add(Dense(96, activation=\"relu\", kernel_regularizer=l2(hp.Float(\"l2_2\", min_value=1e-5, max_value=1e-2, sampling=\"log\"))))\n",
    "    model.add(Dense(32, activation=\"relu\", kernel_regularizer=l2(hp.Float(\"l2_3\", min_value=1e-5, max_value=1e-2, sampling=\"log\"))))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\", AUC(name=\"auc\")],\n",
    "        optimizer=Adam(learning_rate=0.004782203154550918, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_auc\",\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory=\"hide/training history\",\n",
    "    project_name=\"l2_tuning_v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 4\n",
      "l2_0 (Float)\n",
      "{'default': 1e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "l2_1 (Float)\n",
      "{'default': 1e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "l2_2 (Float)\n",
      "{'default': 1e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "l2_3 (Float)\n",
      "{'default': 1e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 09m 04s]\n",
      "val_auc: 0.8137386441230774\n",
      "\n",
      "Best val_auc So Far: 0.8168774545192719\n",
      "Total elapsed time: 01h 23m 28s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=50, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'l2_0': 9.315505072185872e-05, 'l2_1': 0.0008294173681456724, 'l2_2': 0.0001065019888303519, 'l2_3': 0.0006633603807097294}\n"
     ]
    }
   ],
   "source": [
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best hyperparameters:\", best_hp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in hide/training history/l2_tuning_v2\n",
      "Showing 1 best trials\n",
      "Objective(name=\"val_auc\", direction=\"max\")\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "l2_0: 9.315505072185872e-05\n",
      "l2_1: 0.0008294173681456724\n",
      "l2_2: 0.0001065019888303519\n",
      "l2_3: 0.0006633603807097294\n",
      "Score: 0.8168774545192719\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "training_accuracy = best_trial.metrics.get_last_value(\"accuracy\")\n",
    "validation_accuracy = best_trial.metrics.get_last_value(\"val_accuracy\")\n",
    "training_auc = best_trial.metrics.get_last_value(\"auc\")\n",
    "validation_auc = best_trial.metrics.get_last_value(\"val_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC: 0.8262, Accuracy: 0.7571\n",
      "Validation AUC: 0.8169, Accuracy: 0.6785\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training AUC: {round(training_auc, 4)}, Accuracy: {round(training_accuracy, 4)}\")\n",
    "print(f\"Validation AUC: {round(validation_auc, 4)}, Accuracy: {round(validation_accuracy, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Observation: After adding the regularisation, the training AUC and accuracy dropped. The validation AUC slightly increased, but the validation accuracy significantly decreased.\n",
    "* Possible reason: The training dataset was resampled using BorderlineSMOTE, which balances the classes. However, the validation and test sets remain stratified splits of the original data, which might still be imbalanced. When the modelâ€™s probability outputs are less confident due to regularization, the default threshold of accuracy i.e. 0.5 might misclassify more instances, especially in an imbalanced setting.\n",
    "* We may not need to take further action with the following reasons:\n",
    "    1. AUC is robust to class imbalance, while accuracy is not. AUC is chosen as the primary metric.\n",
    "    2. The model without regularisation has a large gap between training AUC (0.9226) and validation AUC (0.8038). It suggests overfitting. The model with regularisation narrows this gap (0.8262 vs. 0.8169), which is a sign of better generalization, but at the cost of overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.hypermodel.build(best_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy, test_auc = best_model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Results for Best Model with L2 Regularization:\n",
      "Test AUC: 0.8098, Accuracy: 0.6886\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Set Results for Best Model with L2 Regularization:\")\n",
    "print(f\"Test AUC: {round(test_auc, 4)}, Accuracy: {round(test_accuracy, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compared to the neural network fitted by Xie et al (2019), my NN has only slightly higher test AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "* Xie, Zidian, et al. \"Building risk prediction models for type 2 diabetes using machine learning techniques.\" Preventing chronic disease 16 (2019): E130."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
