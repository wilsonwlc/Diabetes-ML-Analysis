# XGBoost
XGBoost, short for eXtreme Gradient Boosting, is to fit a series of weak learners, often decision trees, to the residuals of the previous model.

## Algorithm
- Given a dataset $\{ x^{(i)}, y^{(i)} \}^{n}_{i=1}$ and the prediction $y^{(i)}$ is initialised to be 0.5

- At the stage $m \in \{1, 2, ..., M\}$, the objective function is $$\begin{align*}L_{m}&= \sum\limits_{i=1}^{n} L(y^{(i)}, \hat{y}^{(i)}_{m-1}+f_{m}(x^{(i)})) + \Omega(f_{m}) \\ &= \sum\limits_{i=1}^{n} L(y^{(i)}, \hat{y}^{(i)}_{m-1}+f_{m}(x^{(i)})) + \gamma T + \frac{1}{2}\lambda \sum\limits_{t=1}^{T} w_{t}^{2} \end{align*}$$
	- $L$ is the loss function
		- It represents the training loss measuring how well model fit on training data, and can be interpreted as bias reduction
		- For classification: log loss will be used
		- For regression: squared loss function will used
	- $y^{(i)}$ is the true label for the $i^{th}$ instance
	- $\hat{y}_{m-1}^{(i)}$ is the predicted value for the $i^{th}$ instance at iteration $(m-1)$
	- $f_{m}(x^{(i)})$ is the output of the $m^{th}$ weak tree / base learner for the $i^{th}$ instance
		- i.e. minimise residual iteratively
	- $\Omega(f_{m})$ is the regularization term penalising the complexity of the tree and can be interpreted as variance reduction
	- $T$ is the number of leaves in the $f_{m}$
	- $w_{t}$ is the output / prediction for leaf node $t$ of $f_{m}$
	- $\gamma$ and $\lambda$ are regularisation parameters

- By Taylor expansion, the object function can be approximated by $$L_{m}\approx \sum\limits_{i=1}^{n}\left[L(y^{(i)}, \hat{y}_{m-1}^{(i)})+g_{i}f_{m}(x^{(i)})+\frac{1}{2} h_{i}f_{m}^{2}(x^{(i)})\right]+\Omega(f_{m})$$
	- $g_{i} = \frac{\partial L(y^{(i)}, \hat{y}^{(i)}_{m-1})}{\partial \hat{y}^{(i)}_{m-1}}$
	- $h_{i} = \frac{\partial^{2} L(y^{(i)}, \hat{y}^{(i)}_{m-1})}{\partial (\hat{y}^{(i)}_{m-1})^{2}}$
	- For regression, $g_i=-(y^{(i)} - \hat{y}_{m-1}^{(i)})$ and $h_{i} = 1$
		- $\hat{y}_{m-1}^{(i)}$ is the previous probability for $i^{th}$ instance
	- For classification, $g_{i}=-(y_{i}-\hat{y}_{m-1}^{(i)})$ and $h_{i}=\hat{y}_{m-1}^{(i)} \times (1-\hat{y}_{m-1}^{(i)})$

- Remove the constants in the objective function $$\widetilde{L}_{m} = \sum\limits_{i=1}^{n} \left[g_{i}f_{m}(x^{(i)}) + \frac{1}{2} h_{i} f_{m}^{2}(x^{(i)})\right] + \Omega(f_{m})$$
	- Since $\hat{y}_{m-1}^{(i)}$ can be obtained from previous iteration, $L(y^{(i)}, \hat{y}_{m-1}^{(i)})$ is known
- Rewrite the objective function $$\begin{align*} \widetilde{L}_{m} &= \sum\limits_{i=1}^{n} \left[g_{i}f_{m}(x^{(i)}) + \frac{1}{2} h_{i} f_{m}^{2}(x^{(i)})\right] + \left[ \gamma T + \frac{1}{2}\lambda \sum\limits_{t=1}^{T} w_{t}^{2} \right] \\ &= \sum\limits_{t=1}^{T} \left[\left(\sum\limits_{i \in I_{t}}g_{i}\right)w_{t} + \frac{1}{2}\left( \sum\limits_{i \in I_{t}} h_{i} + \lambda \right) w_{t}^{2} \right] + \gamma T \end{align*}$$
	- Partition the dataset into disjoint leaf nodes and group the instances to $I_{t}$, the set of instance belonging to leaf node $t$ of the tree $f_{m}$
	- Recall: $w_{t}$ is the output / prediction for leaf node $t$ of $f_{m}$
	- This is a sum of $T$ independent quadratic functions
- The objective function is minimised when $$ w^{*}_{t} =- \frac{\sum\limits_{i \in I_{t}}g_{i}}{\sum\limits_{i \in I_{t}}h_{i} + \lambda}$$
	- The second derivative of $\widetilde{L}_{m}$ is positive
	- For regression, $\frac{\text{sum of residual}}{\text{Number of residual} + \lambda}$
	- For classification,  $\frac{\text{sum of residual}}{\text{sum of (previous probability * (1- previous probability))}+\lambda}$
- The minimised objective function is  $$\widetilde{L}^{*}_{m} = -\frac{1}{2} \sum\limits_{t=1}^{T} \frac{( \sum\limits_{i \in I_{t}} g_{i} )^{2}}{\sum\limits_{i \in I_{t}} h_{i} + \lambda} + \gamma T$$
- There are infinitely many ways to build the tree to achieve the optimal value of the objective function, so we build it greedily and select the one with largest loss reduction after split i.e. gain
- For each leaf node $t$, the loss reduction after the split into nodes $L$ and $R$ is $$\widetilde{L}_{split} =  \frac{1}{2} \left[\frac{(\sum\limits_{i \in I_{L}}g_{i})^{2}}{\sum\limits_{i \in I_{L}}h_{i} + \lambda} + \frac{(\sum\limits_{i \in I_{R}}g_{i})^{2}}{\sum\limits_{i \in I_{R}}h_{i} + \lambda} - \frac{(\sum\limits_{i \in I_{L}}g_{i} + \sum\limits_{i \in I_{R}}g_{i})^{2}}{\sum\limits_{i \in I_{L}}h_{i} + \sum\limits_{i \in I_{R}}h_{i} + \lambda} \right] -\gamma$$
	- The loss reduction = loss function without split - total loss function with split
- Ignore the constant $$\text{Gain} = \frac{(\sum\limits_{i \in I_{L}}g_{i})^{2}}{\sum\limits_{i \in I_{L}}h_{i} + \lambda} + \frac{(\sum\limits_{i \in I_{R}}g_{i})^{2}}{\sum\limits_{i \in I_{R}}h_{i} + \lambda} - \frac{(\sum\limits_{i \in I_{L}}g_{i} + \sum\limits_{i \in I_{R}}g_{i})^{2}}{\sum\limits_{i \in I_{L}}h_{i} + \sum\limits_{i \in I_{R}}h_{i} + \lambda} - \gamma$$
- Update the predicted value $$\hat{y}_{m}^{(i)} = \hat{y}_{m-1}^{(i)} + \eta f_{m}(x^{(i)})$$
	- $\eta$ is the learning rate
- The final prediction is obtained by summing the predictions of all trees $$\hat{y}^{(i)} = \hat{y}_{M}^{(i)} $$